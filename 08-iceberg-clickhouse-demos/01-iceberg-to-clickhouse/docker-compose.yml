x-image: &image
  image: ${RW_IMAGE:-risingwavelabs/risingwave:v2.6.0}

x-lakekeeper-image: &lakekeeper-image
  image: ${LAKEKEEPER__SERVER_IMAGE:-quay.io/lakekeeper/catalog:latest-main}

x-lakekeeper-environment: &lakekeeper-environment
  - LAKEKEEPER__PG_ENCRYPTION_KEY=This-is-NOT-Secure!
  - LAKEKEEPER__PG_DATABASE_URL_READ=postgresql://postgres:postgres@lakekeeper-db:5432/postgres
  - LAKEKEEPER__PG_DATABASE_URL_WRITE=postgresql://postgres:postgres@lakekeeper-db:5432/postgres
  - LAKEKEEPER__AUTHZ_BACKEND=allowall
  # Externally taken from environment variables if set
  - LAKEKEEPER__OPENID_PROVIDER_URI
  - LAKEKEEPER__OPENID_AUDIENCE
  - LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS
  - LAKEKEEPER__UI__OPENID_CLIENT_ID
  - LAKEKEEPER__UI__OPENID_SCOPE

services:
  # RisingWave Services - Distributed Mode
  compactor-0:
    <<: *image
    command:
      - compactor-node
      - "--listen-addr"
      - "0.0.0.0:6660"
      - "--advertise-addr"
      - "compactor-0:6660"
      - "--prometheus-listener-addr"
      - "0.0.0.0:1260"
      - "--meta-address"
      - "http://meta-node-0:5690"
      - "--config-path"
      - /risingwave.toml
    expose:
      - "6660"
      - "1260"
    ports: []
    depends_on:
      - meta-node-0
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
    container_name: compactor-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/6660; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  compactor-1:
    <<: *image
    command:
      - compactor-node
      - "--listen-addr"
      - "0.0.0.0:6661"
      - "--advertise-addr"
      - "compactor-1:6661"
      - "--prometheus-listener-addr"
      - "0.0.0.0:1261"
      - "--meta-address"
      - "http://meta-node-0:5690"
      - "--compactor-mode"
      - "dedicated-iceberg"
      - "--config-path"
      - /risingwave.toml
    expose:
      - "6661"
      - "1261"
    ports: [ ]
    depends_on:
      - meta-node-0
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
    container_name: compactor-1
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/6661; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 8G

  compute-node-0:
    <<: *image
    command:
      - compute-node
      - "--listen-addr"
      - "0.0.0.0:5688"
      - "--advertise-addr"
      - "compute-node-0:5688"
      - "--prometheus-listener-addr"
      - "0.0.0.0:1222"
      - "--meta-address"
      - "http://meta-node-0:5690"
      - "--config-path"
      - /risingwave.toml
    expose:
      - "5688"
      - "1222"
    ports: []
    depends_on:
      - meta-node-0
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
    container_name: compute-node-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5688; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 8G

  frontend-node-0:
    <<: *image
    command:
      - frontend-node
      - "--listen-addr"
      - "0.0.0.0:4566"
      - "--meta-addr"
      - "http://meta-node-0:5690"
      - "--advertise-addr"
      - "frontend-node-0:4566"
      - "--config-path"
      - /risingwave.toml
      - "--prometheus-listener-addr"
      - "0.0.0.0:2222"
    expose:
      - "4566"
    ports:
      - "4566:4566"
    depends_on:
      - meta-node-0
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
    container_name: frontend-node-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c '> /dev/tcp/127.0.0.1/4566; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  meta-node-0:
    <<: *image
    command:
      - meta-node
      - "--listen-addr"
      - "0.0.0.0:5690"
      - "--advertise-addr"
      - "meta-node-0:5690"
      - "--dashboard-host"
      - "0.0.0.0:5691"
      - "--prometheus-host"
      - "0.0.0.0:1250"
      - "--prometheus-endpoint"
      - "http://prometheus-0:9500"
      - "--backend"
      - sql
      - "--sql-endpoint"
      - "postgres://postgres:@postgres-0:5432/metadata"
      - "--state-store"
      - "hummock+minio://hummockadmin:hummockadmin@minio-0:9301/hummock001"
      - "--data-directory"
      - "hummock_001"
      - "--config-path"
      - /risingwave.toml
    expose:
      - "5690"
      - "1250"
      - "5691"
    ports:
      - "5690:5690"
      - "5691:5691"
    depends_on:
      - "postgres-0"
      - "minio-0"
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
      RW_TELEMETRY_TYPE: ${RW_TELEMETRY_TYPE:-"docker-compose"}
      RW_SECRET_STORE_PRIVATE_KEY_HEX: ${RW_SECRET_STORE_PRIVATE_KEY_HEX:-0123456789abcdef0123456789abcdef}
    container_name: meta-node-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  
  postgres-0:
    image: "postgres:15-alpine"
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
      - POSTGRES_USER=postgres
      - POSTGRES_DB=metadata
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    expose:
      - "5432"
    ports:
      - "8432:5432"
    volumes:
      - "postgres-0:/var/lib/postgresql/data"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 2s
      timeout: 5s
      retries: 5
    restart: always

  minio-0:
    image: "quay.io/minio/minio:latest"
    command:
      - server
      - "--address"
      - "0.0.0.0:9301"
      - "--console-address"
      - "0.0.0.0:9400"
      - /data
    expose:
      - "9301"
      - "9400"
    ports:
      - "9301:9301"
      - "9400:9400"
    volumes:
      - "minio-0:/data"
    entrypoint: |
      /bin/sh -c '
      set -e
      echo "127.0.0.1 minio-0" | tee -a /etc/hosts
      mkdir -p "/data/hummock001"

      /usr/bin/docker-entrypoint.sh "$$0" "$$@"
      '
    environment:
      MINIO_CI_CD: "1"
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: "http://prometheus-0:9500"
      MINIO_ROOT_PASSWORD: hummockadmin
      MINIO_ROOT_USER: hummockadmin
      MINIO_DOMAIN: "minio-0"
    container_name: minio-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf "GET / HTTP/1.1\n\n" > /dev/tcp/127.0.0.1/9301; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  # Lakekeeper Services
  lakekeeper:
    <<: *lakekeeper-image
    pull_policy: missing
    environment: *lakekeeper-environment
    command: [ "serve" ]
    healthcheck:
      test: [ "CMD", "/home/nonroot/lakekeeper", "healthcheck" ]
      interval: 1s
      timeout: 10s
      retries: 3
      start_period: 3s
    depends_on:
      lakekeeper-migrate:
        condition: service_completed_successfully
    ports:
      - "8181:8181"
    container_name: lakekeeper


  # Bootstrap Lakekeeper after it is healthy and create the warehouse automatically
  lakekeeper-bootstrap:
    image: curlimages/curl:8.10.1
    restart: "no"
    depends_on:
      lakekeeper:
        condition: service_healthy
      minio-0:
        condition: service_healthy
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        set -e
        echo "Bootstrapping Lakekeeper (idempotent)…"
        # Accept terms of use (safe to retry)
        curl -sS -X POST http://lakekeeper:8181/management/v1/bootstrap \
          -H 'Content-Type: application/json' \
          -d '{"accept-terms-of-use": true}' || true

        echo "Creating Lakekeeper warehouse 'risingwave-warehouse' (idempotent)…"
        curl -sS -X POST http://lakekeeper:8181/management/v1/warehouse \
          -H 'Content-Type: application/json' \
          -d '{
            "warehouse-name": "risingwave-warehouse",
            "delete-profile": { "type": "hard" },
            "storage-credential": {
              "type": "s3",
              "credential-type": "access-key",
              "aws-access-key-id": "hummockadmin",
              "aws-secret-access-key": "hummockadmin"
            },
            "storage-profile": {
              "type": "s3",
              "bucket": "hummock001",
              "region": "us-east-1",
              "flavor": "s3-compat",
              "endpoint": "http://minio-0:9301",
              "path-style-access": true,
              "sts-enabled": false,
              "key-prefix": "risingwave-lakekeeper"
            }
          }' || true
        echo "Lakekeeper bootstrap complete."


  lakekeeper-migrate:
    <<: *lakekeeper-image
    pull_policy: missing
    environment: *lakekeeper-environment
    restart: "no"
    command: [ "migrate" ]
    depends_on:
      lakekeeper-db:
        condition: service_healthy
    container_name: lakekeeper-migrate

  lakekeeper-db:
    image: "postgres:16-alpine"
    environment:
      - POSTGRES_PASSWORD=postgres
      - PGDATA=/var/lib/postgresql/data/pgdata
    expose:
      - "5432"
    ports:
      - "8433:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres -p 5432 -d postgres" ]
      interval: 2s
      timeout: 10s
      retries: 2
      start_period: 10s
    restart: always
    volumes:
      - "lakekeeper-db:/var/lib/postgresql/data/pgdata"
    container_name: lakekeeper-db
  
  clickhouse-server:
    image: clickhouse/clickhouse-server:25.9
    container_name: clickhouse-server-1
    hostname: clickhouse-server-1
    ports:
      - "8123:8123"
      - "9000:9000"
      - "9004:9004"
    expose:
      - 9009
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    environment:
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: "default"
  
  message_queue:
    image: "redpandadata/redpanda:latest"
    command:
      - redpanda
      - start
      - "--smp"
      - "1"
      - "--reserve-memory"
      - 0M
      - "--memory"
      - 4G
      - "--overprovisioned"
      - "--node-id"
      - "0"
      - "--check=false"
      - "--kafka-addr"
      - "PLAINTEXT://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092"
      - "--advertise-kafka-addr"
      - "PLAINTEXT://message_queue:29092,OUTSIDE://localhost:9092"
    expose:
      - "29092"
      - "9092"
      - "9644"
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9644:9644"
      - "8081:8081"
    depends_on: [ ]
    volumes:
      - "message_queue:/var/lib/redpanda/data"
    environment: {}
    container_name: message_queue
    healthcheck:
      test: curl -f localhost:9644/v1/status/ready
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  # Apache Spark with Iceberg pre-configured to use Lakekeeper REST catalog
  spark-iceberg:
    image: tabulario/spark-iceberg:3.5.5_1.8.1
    container_name: spark-iceberg
    depends_on:
      lakekeeper:
        condition: service_healthy
      minio-0:
        condition: service_healthy
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master
      - "4040:4040"  # Spark Application Web UI
      - "10000:10000"  # Thrift Server
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        set -e
        rm -rf /opt/spark/conf
        mkdir -p /opt/spark/conf
        cat > /opt/spark/conf/spark-defaults.conf <<'EOF'
        spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.lake=org.apache.iceberg.spark.SparkCatalog
        spark.sql.catalog.lake.type=rest
        spark.sql.catalog.lake.uri=http://lakekeeper:8181/catalog/
        spark.sql.catalog.lake.warehouse=risingwave-warehouse
        spark.sql.catalog.lake.io-impl=org.apache.iceberg.aws.s3.S3FileIO
        spark.sql.catalog.lake.s3.endpoint=http://minio-0:9301
        spark.sql.catalog.lake.s3.path-style-access=true
        spark.sql.catalog.lake.s3.access-key-id=hummockadmin
        spark.sql.catalog.lake.s3.secret-access-key=hummockadmin
        spark.sql.catalog.lake.s3.region=us-east-1
        EOF
        echo "Spark configured for Iceberg Lakekeeper catalog. Starting Spark services..."
        
        start-master.sh -p 7077
        start-worker.sh spark://spark-iceberg:7077
        
        start-history-server.sh
        start-thriftserver.sh --driver-java-options "-Dderby.system.home=/tmp/derby"
        
        echo "Spark services started. You can access:"
        echo "  - Spark Master UI: http://localhost:8080"
        echo "  - Spark Application UI: http://localhost:4040"
        echo "  - Thrift Server: localhost:10000"
        echo ""
        echo "To use spark-sql, run:"
        echo "  docker exec -it spark-iceberg spark-sql"
        
        tail -f /dev/null

volumes:
  postgres-0:
    external: false
  minio-0:
    external: false
  lakekeeper-db:
    external: false
  clickhouse_data:
    external: false
  message_queue:
    external: false